{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"7vEBNTHjuvon"},"source":["```text\n","SPDX-FileCopyrightText: 2023 Google LLC\n","SPDX-License-Identifier: Apache-2.0\n","```\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"yxKFLozC5tfO"},"source":["# ü§Ø Using the Natural Language API with Python\n","\n","<center>\n","<table><tr><td>\n","<img src=\"pics/natural_language_api.png\" style=\"height:200px\" height=\"200\" />\n","</td></tr></table>\n","<table><tr>\n","<td><a href=\"https://colab.research.google.com/github/PicardParis/cloud-snippets/blob/main/python/colab/Using the Natural Language API with Python.ipynb\">\n","<img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\" align=\"center\"> Open in Colab\n","</a></td>\n","<td><a href=\"https://github.com/PicardParis/cloud-snippets/blob/main/python/colab/Using the Natural Language API with Python.ipynb\">\n","<img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\" align=\"center\"> View on GitHub\n","</a></td>\n","</tr></table>\n","</center>\n","\n","The [Natural Language API](https://cloud.google.com/natural-language/docs/) lets you extract information from unstructured text using Google machine learning. In this tutorial, you'll focus on using its Python client library to perform the following:\n","\n","- Sentiment analysis\n","- Entity analysis\n","- Syntax analysis\n","- Content classification\n","- Text moderation\n","\n","This notebook requires a Google Cloud project:\n","\n","- If needed, [create a new Google Cloud project](https://console.cloud.google.com/cloud-resource-manager).\n","- Make sure that billing is enabled for your project.\n","- It uses billable services but not should generate any cost (see the Natural Language API [free monthly thresholds](https://cloud.google.com/natural-language/pricing)).\n","\n","It can run in autopilot mode:\n","\n","- Define your project ID below.\n","- Launch \"_Runtime_\" > \"_Run all_\".\n","- If the setup step installs packages and restarts, launch \"_Run all_\" again.\n","- The first time, you'll need to allow access to your Google credentials. Select your Google Cloud account.\n","\n","> This port to Colab was originally published on [Google Developers Codelabs](https://codelabs.developers.google.com/codelabs/cloud-natural-language-python3).\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"JAUW8f9oi8mi"},"source":["---\n","\n","## ‚úîÔ∏è Setup\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mn2UJ8_VUtFa"},"outputs":[],"source":["# @title üì¶Ô∏è Packages (may restart) {display-mode: \"form\"}\n","\n","import sys\n","from importlib import metadata\n","\n","from IPython.core.getipython import get_ipython\n","from packaging import version\n","\n","# Services needed for this lab (with minimum version)\n","# Note: This assumes that earlier versions are non-breaking\n","GOOGLE_CLOUD_SERVICES = [\n","    (\"language\", \"2.10.0\"),\n","]\n","APIS = [f\"{service}.googleapis.com\" for service, _ in GOOGLE_CLOUD_SERVICES]\n","\n","# Check runtime\n","running_in_colab = \"google.colab\" in sys.modules\n","assert running_in_colab, \"‚ùå The notebook was not tested outside of Colab\"\n","print(\"‚úîÔ∏è Running in Colab\")\n","\n","# Check packages\n","packages = []\n","for service, min_version_str in GOOGLE_CLOUD_SERVICES:\n","    package = f\"google-cloud-{service}\"\n","    min_version = version.parse(min_version_str)\n","    try:\n","        lib = f\"google.cloud.{service}\"\n","        lib_version = version.parse(metadata.version(lib))\n","        if min_version <= lib_version:\n","            print(f\"‚úîÔ∏è {lib}=={lib_version!s}\")\n","            continue\n","        packages.append(package)\n","        print(f\"üì¶Ô∏è {package} to be updated‚Ä¶\")\n","    except metadata.PackageNotFoundError:\n","        packages.append(package)\n","        print(f\"üì¶Ô∏è {package} to be installed‚Ä¶\")\n","\n","if packages:\n","    # Install and restart\n","    requirements = \" \".join(packages)\n","    %pip install --upgrade $requirements --quiet\n","    if instance := get_ipython():\n","        instance.kernel.do_shutdown(True)\n","    raise RuntimeWarning(\"üîÑ Restarting‚Ä¶ (run the cell again)\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Df7bSS-b7rdW"},"outputs":[],"source":["# @title ‚öôÔ∏è Project ID {display-mode: \"form\"}\n","\n","PROJECT_ID = \"\"  # @param {type:\"string\"}\n","\n","assert PROJECT_ID, \"‚ùå Please enter your project ID\"\n","\n","print(f'‚úîÔ∏è PROJECT_ID: \"{PROJECT_ID}\"')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5Xt81s9pOlfI"},"outputs":[],"source":["# @title üîë Authentication {display-mode: \"form\"}\n","from google.colab import auth\n","\n","auth.authenticate_user(project_id=PROJECT_ID)\n","print(f\"‚úîÔ∏è Authenticated\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w-QXa96aXzgw"},"outputs":[],"source":["# @title üîì Project APIs {display-mode: \"form\"}\n","res = !gcloud services list --enabled --format \"value(config.name)\"\n","\n","apis_to_enable = \"\"\n","for api in APIS:\n","    if api in res:\n","        print(f'‚úîÔ∏è API \"{api}\" is enabled')\n","    else:\n","        apis_to_enable += f\"{api} \"\n","\n","if apis_to_enable:\n","    print(f'üîì Enabling API \"{apis_to_enable}\"‚Ä¶')\n","    !gcloud services enable $api\n","elif not APIS:\n","    print(f\"‚úîÔ∏è No specific API needed\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8zA4DxGDagbv"},"outputs":[],"source":["# @title üõ†Ô∏è Helpers {display-mode: \"form\"}\n","import pandas as pd\n","from IPython.display import display\n","\n","\n","def show_table(columns, data, formats=None, remove_empty_columns=False):\n","    df = pd.DataFrame(columns=columns, data=data)\n","    if remove_empty_columns:\n","        empty_cols = [col for col in df if df[col].eq(\"\").all()]\n","        df.drop(empty_cols, axis=1, inplace=True)\n","    # Customize formatting\n","    styler = df.style\n","    if formats:\n","        styler.format(formats)\n","    # Left-align string columns\n","    df = df.convert_dtypes()\n","    str_cols = list(df.select_dtypes(\"string\").keys())\n","    styler = styler.set_properties(subset=str_cols, **{\"text-align\": \"left\"})\n","    # Center headers\n","    styler.set_table_styles([{\"selector\": \"th\", \"props\": [(\"text-align\", \"center\")]}])\n","    styler.hide()\n","    display(styler)\n","\n","\n","print(f\"‚úîÔ∏è Helpers defined\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"YuhPWlD5M1Kp"},"source":["---\n","\n","## üêç Using the Python client library\n","\n","You can use the Natural Language API in Python with the client library `google-cloud-language`. The previous step already checked its installation. Import the client library:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vKysAffRWGnv"},"outputs":[],"source":["from google.cloud import language"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"o83udK7BZOBC"},"source":["---\n","\n","## 1Ô∏è‚É£ Sentiment analysis\n","\n","Sentiment analysis inspects the given text and identifies the prevailing emotional opinions within the text, especially to determine expressed sentiments as positive, negative, or neutral, both at the sentence and the document levels. It is performed with the `analyze_sentiment` method which returns an `AnalyzeSentimentResponse`.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ELEmIsYJZQ-i"},"outputs":[],"source":["def analyze_text_sentiment(text: str) -> language.AnalyzeSentimentResponse:\n","    client = language.LanguageServiceClient()\n","    document = language.Document(\n","        content=text,\n","        type_=language.Document.Type.PLAIN_TEXT,\n","    )\n","    return client.analyze_sentiment(document=document)\n","\n","\n","def show_text_sentiment(response: language.AnalyzeSentimentResponse):\n","    columns = [\"score\", \"sentence\"]\n","    data = [(s.sentiment.score, s.text.content) for s in response.sentences]\n","    formats = {\"score\": \"{:+.1f}\"}\n","    print(\"At sentence level:\")\n","    show_table(columns, data, formats)\n","\n","    sentiment = response.document_sentiment\n","    columns = [\"score\", \"magnitude\", \"language\"]\n","    data = [(sentiment.score, sentiment.magnitude, response.language)]\n","    formats = {\"score\": \"{:+.1f}\", \"magnitude\": \"{:.1f}\"}\n","    print(\"\")\n","    print(\"At document level:\")\n","    show_table(columns, data, formats)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"AB2X-SKMwjLU"},"source":["Now, let's perform an analysis:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":259},"executionInfo":{"elapsed":789,"status":"ok","timestamp":1686649993271,"user":{"displayName":"Laurent Picard (GCP Demos)","userId":"15587849602996756367"},"user_tz":-120},"id":"M9mzah76Y0gl","outputId":"07529a73-5b28-4b4f-be6f-315af93497e4"},"outputs":[{"name":"stdout","output_type":"stream","text":["At sentence level:\n"]},{"data":{"text/html":["<style type=\"text/css\">\n","#T_5ebd3 th {\n","  text-align: center;\n","}\n","#T_5ebd3_row0_col1, #T_5ebd3_row1_col1, #T_5ebd3_row2_col1 {\n","  text-align: left;\n","}\n","</style>\n","<table id=\"T_5ebd3\" class=\"dataframe\">\n","  <thead>\n","    <tr>\n","      <th id=\"T_5ebd3_level0_col0\" class=\"col_heading level0 col0\" >score</th>\n","      <th id=\"T_5ebd3_level0_col1\" class=\"col_heading level0 col1\" >sentence</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td id=\"T_5ebd3_row0_col0\" class=\"data row0 col0\" >+0.8</td>\n","      <td id=\"T_5ebd3_row0_col1\" class=\"data row0 col1\" >Python is a very readable language, which makes it easy to understand and maintain code.</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_5ebd3_row1_col0\" class=\"data row1 col0\" >+0.9</td>\n","      <td id=\"T_5ebd3_row1_col1\" class=\"data row1 col1\" >It's simple, very flexible, easy to learn, and suitable for a wide variety of tasks.</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_5ebd3_row2_col0\" class=\"data row2 col0\" >-0.4</td>\n","      <td id=\"T_5ebd3_row2_col1\" class=\"data row2 col1\" >One disadvantage is its speed: it's not as fast as some other programming languages.</td>\n","    </tr>\n","  </tbody>\n","</table>\n"],"text/plain":["<pandas.io.formats.style.Styler at 0x7f3094196b00>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","At document level:\n"]},{"data":{"text/html":["<style type=\"text/css\">\n","#T_208fa th {\n","  text-align: center;\n","}\n","#T_208fa_row0_col2 {\n","  text-align: left;\n","}\n","</style>\n","<table id=\"T_208fa\" class=\"dataframe\">\n","  <thead>\n","    <tr>\n","      <th id=\"T_208fa_level0_col0\" class=\"col_heading level0 col0\" >score</th>\n","      <th id=\"T_208fa_level0_col1\" class=\"col_heading level0 col1\" >magnitude</th>\n","      <th id=\"T_208fa_level0_col2\" class=\"col_heading level0 col2\" >language</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td id=\"T_208fa_row0_col0\" class=\"data row0 col0\" >+0.4</td>\n","      <td id=\"T_208fa_row0_col1\" class=\"data row0 col1\" >2.2</td>\n","      <td id=\"T_208fa_row0_col2\" class=\"data row0 col2\" >en</td>\n","    </tr>\n","  </tbody>\n","</table>\n"],"text/plain":["<pandas.io.formats.style.Styler at 0x7f3094196e30>"]},"metadata":{},"output_type":"display_data"}],"source":["# Input\n","text = \"Python is a very readable language, which makes it easy to understand and maintain code. It's simple, very flexible, easy to learn, and suitable for a wide variety of tasks. One disadvantage is its speed: it's not as fast as some other programming languages.\"  # @param {type:\"string\"}\n","\n","# Send a request to the API\n","analyze_sentiment_response = analyze_text_sentiment(text)\n","\n","# Show the results\n","show_text_sentiment(analyze_sentiment_response)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"7hU8WO15c0jn"},"source":["Notes:\n","\n","- For information on which languages are supported by the Natural Language API, see [Language Support](https://cloud.google.com/natural-language/docs/languages#sentiment_analysis).\n","- The `score` of the sentiment ranges between -1.0 (negative) and 1.0 (positive) and corresponds to the overall sentiment from the given information.\n","- The `magnitude` of the sentiment ranges from 0.0 to +infinity and indicates the overall strength of sentiment from the given information. The more information provided, the higher the magnitude.\n","- For more information on how to interpret the `score` and `magnitude` sentiment values included in the analysis, see [Interpreting sentiment analysis values](https://cloud.google.com/natural-language/docs/basics#interpreting_sentiment_analysis_values).\n","- Each API response returns the document automatically-detected language (in ISO-639-1). It is shown here and will be skipped in the next analysis examples.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"HHY_gesZdrM1"},"source":["---\n","\n","## 2Ô∏è‚É£ Entity analysis\n","\n","Entity analysis inspects the given text for known entities (proper nouns such as public figures, landmarks, etc.), and returns information about those entities. It is performed with the `analyze_entities` method which returns an `AnalyzeEntitiesResponse`.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6NOJl4LQdrM8"},"outputs":[],"source":["def analyze_text_entities(text: str) -> language.AnalyzeEntitiesResponse:\n","    client = language.LanguageServiceClient()\n","    document = language.Document(\n","        content=text,\n","        type_=language.Document.Type.PLAIN_TEXT,\n","    )\n","    return client.analyze_entities(document=document)\n","\n","\n","def show_text_entities(response: language.AnalyzeEntitiesResponse):\n","    columns = (\"name\", \"type\", \"salience\", \"mid\", \"wikipedia_url\")\n","    data = (\n","        (\n","            entity.name,\n","            entity.type_.name,\n","            entity.salience,\n","            entity.metadata.get(\"mid\", \"\"),\n","            entity.metadata.get(\"wikipedia_url\", \"\"),\n","        )\n","        for entity in response.entities\n","    )\n","    formats = {\"salience\": \"{:.1%}\"}\n","    show_table(columns, data, formats)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"tdiVhvLrxJx7"},"source":["Now, let's perform an analysis:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":269},"executionInfo":{"elapsed":799,"status":"ok","timestamp":1686649994068,"user":{"displayName":"Laurent Picard (GCP Demos)","userId":"15587849602996756367"},"user_tz":-120},"id":"28jLkpSqZ5Fc","outputId":"286ea191-e455-4c62-f037-a6f8b8d79a59"},"outputs":[{"data":{"text/html":["<style type=\"text/css\">\n","#T_205da th {\n","  text-align: center;\n","}\n","#T_205da_row0_col0, #T_205da_row0_col1, #T_205da_row0_col3, #T_205da_row0_col4, #T_205da_row1_col0, #T_205da_row1_col1, #T_205da_row1_col3, #T_205da_row1_col4, #T_205da_row2_col0, #T_205da_row2_col1, #T_205da_row2_col3, #T_205da_row2_col4, #T_205da_row3_col0, #T_205da_row3_col1, #T_205da_row3_col3, #T_205da_row3_col4, #T_205da_row4_col0, #T_205da_row4_col1, #T_205da_row4_col3, #T_205da_row4_col4, #T_205da_row5_col0, #T_205da_row5_col1, #T_205da_row5_col3, #T_205da_row5_col4, #T_205da_row6_col0, #T_205da_row6_col1, #T_205da_row6_col3, #T_205da_row6_col4 {\n","  text-align: left;\n","}\n","</style>\n","<table id=\"T_205da\" class=\"dataframe\">\n","  <thead>\n","    <tr>\n","      <th id=\"T_205da_level0_col0\" class=\"col_heading level0 col0\" >name</th>\n","      <th id=\"T_205da_level0_col1\" class=\"col_heading level0 col1\" >type</th>\n","      <th id=\"T_205da_level0_col2\" class=\"col_heading level0 col2\" >salience</th>\n","      <th id=\"T_205da_level0_col3\" class=\"col_heading level0 col3\" >mid</th>\n","      <th id=\"T_205da_level0_col4\" class=\"col_heading level0 col4\" >wikipedia_url</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td id=\"T_205da_row0_col0\" class=\"data row0 col0\" >Guido van Rossum</td>\n","      <td id=\"T_205da_row0_col1\" class=\"data row0 col1\" >PERSON</td>\n","      <td id=\"T_205da_row0_col2\" class=\"data row0 col2\" >49.8%</td>\n","      <td id=\"T_205da_row0_col3\" class=\"data row0 col3\" >/m/01h05c</td>\n","      <td id=\"T_205da_row0_col4\" class=\"data row0 col4\" >https://en.wikipedia.org/wiki/Guido_van_Rossum</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_205da_row1_col0\" class=\"data row1 col0\" >Python</td>\n","      <td id=\"T_205da_row1_col1\" class=\"data row1 col1\" >ORGANIZATION</td>\n","      <td id=\"T_205da_row1_col2\" class=\"data row1 col2\" >38.4%</td>\n","      <td id=\"T_205da_row1_col3\" class=\"data row1 col3\" >/m/05z1_</td>\n","      <td id=\"T_205da_row1_col4\" class=\"data row1 col4\" >https://en.wikipedia.org/wiki/Python_(programming_language)</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_205da_row2_col0\" class=\"data row2 col0\" >creator</td>\n","      <td id=\"T_205da_row2_col1\" class=\"data row2 col1\" >PERSON</td>\n","      <td id=\"T_205da_row2_col2\" class=\"data row2 col2\" >5.1%</td>\n","      <td id=\"T_205da_row2_col3\" class=\"data row2 col3\" ></td>\n","      <td id=\"T_205da_row2_col4\" class=\"data row2 col4\" ></td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_205da_row3_col0\" class=\"data row3 col0\" >Monty Python</td>\n","      <td id=\"T_205da_row3_col1\" class=\"data row3 col1\" >PERSON</td>\n","      <td id=\"T_205da_row3_col2\" class=\"data row3 col2\" >3.2%</td>\n","      <td id=\"T_205da_row3_col3\" class=\"data row3 col3\" >/m/04sd0</td>\n","      <td id=\"T_205da_row3_col4\" class=\"data row3 col4\" >https://en.wikipedia.org/wiki/Monty_Python</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_205da_row4_col0\" class=\"data row4 col0\" >comedy troupe</td>\n","      <td id=\"T_205da_row4_col1\" class=\"data row4 col1\" >PERSON</td>\n","      <td id=\"T_205da_row4_col2\" class=\"data row4 col2\" >1.6%</td>\n","      <td id=\"T_205da_row4_col3\" class=\"data row4 col3\" ></td>\n","      <td id=\"T_205da_row4_col4\" class=\"data row4 col4\" ></td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_205da_row5_col0\" class=\"data row5 col0\" >Haarlem</td>\n","      <td id=\"T_205da_row5_col1\" class=\"data row5 col1\" >LOCATION</td>\n","      <td id=\"T_205da_row5_col2\" class=\"data row5 col2\" >1.0%</td>\n","      <td id=\"T_205da_row5_col3\" class=\"data row5 col3\" >/m/0h095</td>\n","      <td id=\"T_205da_row5_col4\" class=\"data row5 col4\" >https://en.wikipedia.org/wiki/Haarlem</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_205da_row6_col0\" class=\"data row6 col0\" >Netherlands</td>\n","      <td id=\"T_205da_row6_col1\" class=\"data row6 col1\" >LOCATION</td>\n","      <td id=\"T_205da_row6_col2\" class=\"data row6 col2\" >0.7%</td>\n","      <td id=\"T_205da_row6_col3\" class=\"data row6 col3\" >/m/059j2</td>\n","      <td id=\"T_205da_row6_col4\" class=\"data row6 col4\" >https://en.wikipedia.org/wiki/Netherlands</td>\n","    </tr>\n","  </tbody>\n","</table>\n"],"text/plain":["<pandas.io.formats.style.Styler at 0x7f309681e980>"]},"metadata":{},"output_type":"display_data"}],"source":["# Input\n","text = \"Guido van Rossum is best known as the creator of Python, which he named after the Monty Python comedy troupe. He was born in Haarlem, Netherlands.\"  # @param {type:\"string\"}\n","\n","# Send a request to the API\n","analyze_entities_response = analyze_text_entities(text)\n","\n","# Show the results\n","show_text_entities(analyze_entities_response)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ByF1lee3vCIE"},"source":["Notes:\n","\n","- For information on which languages are supported by this method, see [Language Support](https://cloud.google.com/natural-language/docs/languages#entity_analysis).\n","- The `type` of the entity is an enum that lets you classify or differentiate entities. For example, this can help distinguish the similarly named entities _‚ÄúT.E. Lawrence‚Äù_ (a `PERSON`) from _‚ÄúLawrence of Arabia‚Äù_ (the film) (tagged as a `WORK_OF_ART`). See [`Entity.Type`](https://cloud.google.com/python/docs/reference/language/latest/google.cloud.language_v1.types.Entity.Type).\n","- The entity `salience` indicates the importance or relevance of this entity to the entire document text. This score can assist information retrieval and summarization by prioritizing salient entities. Scores closer to 0.0 are less important, while scores closer to 1.0 are highly important.\n","- For more information, see [Entity analysis](https://cloud.google.com/natural-language/docs/basics#entity_analysis).\n","- You can also combine both entity analysis and sentiment analysis with the `analyze_entity_sentiment` method. See [Entity sentiment analysis](https://cloud.google.com/natural-language/docs/basics#entity_analysis).\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"uqzc9quwdurW"},"source":["---\n","\n","## 3Ô∏è‚É£ Syntax analysis\n","\n","Syntax analysis extracts linguistic information, breaking up the given text into a series of sentences and tokens (generally based on word boundaries), providing further analysis on those tokens. It is performed with the `analyze_syntax` method which returns an `AnalyzeSyntaxResponse`.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iAOfoJEUdurW"},"outputs":[],"source":["def analyze_text_syntax(text: str) -> language.AnalyzeSyntaxResponse:\n","    client = language.LanguageServiceClient()\n","    document = language.Document(\n","        content=text,\n","        type_=language.Document.Type.PLAIN_TEXT,\n","    )\n","    return client.analyze_syntax(document=document)\n","\n","\n","def get_token_info(token: language.Token | None) -> list[str]:\n","    parts = [\n","        \"tag\",\n","        \"aspect\",\n","        \"case\",\n","        \"form\",\n","        \"gender\",\n","        \"mood\",\n","        \"number\",\n","        \"person\",\n","        \"proper\",\n","        \"reciprocity\",\n","        \"tense\",\n","        \"voice\",\n","    ]\n","    if not token:\n","        return [\"token\", \"lemma\"] + parts\n","\n","    text = token.text.content\n","    lemma = token.lemma if token.lemma != token.text.content else \"\"\n","    info = [text, lemma]\n","    for part in parts:\n","        pos = token.part_of_speech\n","        info.append(getattr(pos, part).name if part in pos else \"\")\n","\n","    return info\n","\n","\n","def show_text_syntax(response: language.AnalyzeSyntaxResponse):\n","    tokens = len(response.tokens)\n","    sentences = len(response.sentences)\n","    columns = get_token_info(None)\n","    data = (get_token_info(token) for token in response.tokens)\n","\n","    print(f\"Analyzed {tokens} token(s) from {sentences} sentence(s)\")\n","    show_table(columns, data, remove_empty_columns=True)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"YSDS0-o0xNqf"},"source":["Now, let's perform an analysis:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":693},"executionInfo":{"elapsed":812,"status":"ok","timestamp":1686649994878,"user":{"displayName":"Laurent Picard (GCP Demos)","userId":"15587849602996756367"},"user_tz":-120},"id":"spjtSAfaDqQL","outputId":"2a52b7ee-6af2-4e88-a8d4-60f82e2d7223"},"outputs":[{"name":"stdout","output_type":"stream","text":["Analyzed 20 token(s) from 2 sentence(s)\n"]},{"data":{"text/html":["<style type=\"text/css\">\n","#T_95b02 th {\n","  text-align: center;\n","}\n","#T_95b02_row0_col0, #T_95b02_row0_col1, #T_95b02_row0_col2, #T_95b02_row0_col3, #T_95b02_row0_col4, #T_95b02_row0_col5, #T_95b02_row0_col6, #T_95b02_row0_col7, #T_95b02_row0_col8, #T_95b02_row0_col9, #T_95b02_row0_col10, #T_95b02_row1_col0, #T_95b02_row1_col1, #T_95b02_row1_col2, #T_95b02_row1_col3, #T_95b02_row1_col4, #T_95b02_row1_col5, #T_95b02_row1_col6, #T_95b02_row1_col7, #T_95b02_row1_col8, #T_95b02_row1_col9, #T_95b02_row1_col10, #T_95b02_row2_col0, #T_95b02_row2_col1, #T_95b02_row2_col2, #T_95b02_row2_col3, #T_95b02_row2_col4, #T_95b02_row2_col5, #T_95b02_row2_col6, #T_95b02_row2_col7, #T_95b02_row2_col8, #T_95b02_row2_col9, #T_95b02_row2_col10, #T_95b02_row3_col0, #T_95b02_row3_col1, #T_95b02_row3_col2, #T_95b02_row3_col3, #T_95b02_row3_col4, #T_95b02_row3_col5, #T_95b02_row3_col6, #T_95b02_row3_col7, #T_95b02_row3_col8, #T_95b02_row3_col9, #T_95b02_row3_col10, #T_95b02_row4_col0, #T_95b02_row4_col1, #T_95b02_row4_col2, #T_95b02_row4_col3, #T_95b02_row4_col4, #T_95b02_row4_col5, #T_95b02_row4_col6, #T_95b02_row4_col7, #T_95b02_row4_col8, #T_95b02_row4_col9, #T_95b02_row4_col10, #T_95b02_row5_col0, #T_95b02_row5_col1, #T_95b02_row5_col2, #T_95b02_row5_col3, #T_95b02_row5_col4, #T_95b02_row5_col5, #T_95b02_row5_col6, #T_95b02_row5_col7, #T_95b02_row5_col8, #T_95b02_row5_col9, #T_95b02_row5_col10, #T_95b02_row6_col0, #T_95b02_row6_col1, #T_95b02_row6_col2, #T_95b02_row6_col3, #T_95b02_row6_col4, #T_95b02_row6_col5, #T_95b02_row6_col6, #T_95b02_row6_col7, #T_95b02_row6_col8, #T_95b02_row6_col9, #T_95b02_row6_col10, #T_95b02_row7_col0, #T_95b02_row7_col1, #T_95b02_row7_col2, #T_95b02_row7_col3, #T_95b02_row7_col4, #T_95b02_row7_col5, #T_95b02_row7_col6, #T_95b02_row7_col7, #T_95b02_row7_col8, #T_95b02_row7_col9, #T_95b02_row7_col10, #T_95b02_row8_col0, #T_95b02_row8_col1, #T_95b02_row8_col2, #T_95b02_row8_col3, #T_95b02_row8_col4, #T_95b02_row8_col5, #T_95b02_row8_col6, #T_95b02_row8_col7, #T_95b02_row8_col8, #T_95b02_row8_col9, #T_95b02_row8_col10, #T_95b02_row9_col0, #T_95b02_row9_col1, #T_95b02_row9_col2, #T_95b02_row9_col3, #T_95b02_row9_col4, #T_95b02_row9_col5, #T_95b02_row9_col6, #T_95b02_row9_col7, #T_95b02_row9_col8, #T_95b02_row9_col9, #T_95b02_row9_col10, #T_95b02_row10_col0, #T_95b02_row10_col1, #T_95b02_row10_col2, #T_95b02_row10_col3, #T_95b02_row10_col4, #T_95b02_row10_col5, #T_95b02_row10_col6, #T_95b02_row10_col7, #T_95b02_row10_col8, #T_95b02_row10_col9, #T_95b02_row10_col10, #T_95b02_row11_col0, #T_95b02_row11_col1, #T_95b02_row11_col2, #T_95b02_row11_col3, #T_95b02_row11_col4, #T_95b02_row11_col5, #T_95b02_row11_col6, #T_95b02_row11_col7, #T_95b02_row11_col8, #T_95b02_row11_col9, #T_95b02_row11_col10, #T_95b02_row12_col0, #T_95b02_row12_col1, #T_95b02_row12_col2, #T_95b02_row12_col3, #T_95b02_row12_col4, #T_95b02_row12_col5, #T_95b02_row12_col6, #T_95b02_row12_col7, #T_95b02_row12_col8, #T_95b02_row12_col9, #T_95b02_row12_col10, #T_95b02_row13_col0, #T_95b02_row13_col1, #T_95b02_row13_col2, #T_95b02_row13_col3, #T_95b02_row13_col4, #T_95b02_row13_col5, #T_95b02_row13_col6, #T_95b02_row13_col7, #T_95b02_row13_col8, #T_95b02_row13_col9, #T_95b02_row13_col10, #T_95b02_row14_col0, #T_95b02_row14_col1, #T_95b02_row14_col2, #T_95b02_row14_col3, #T_95b02_row14_col4, #T_95b02_row14_col5, #T_95b02_row14_col6, #T_95b02_row14_col7, #T_95b02_row14_col8, #T_95b02_row14_col9, #T_95b02_row14_col10, #T_95b02_row15_col0, #T_95b02_row15_col1, #T_95b02_row15_col2, #T_95b02_row15_col3, #T_95b02_row15_col4, #T_95b02_row15_col5, #T_95b02_row15_col6, #T_95b02_row15_col7, #T_95b02_row15_col8, #T_95b02_row15_col9, #T_95b02_row15_col10, #T_95b02_row16_col0, #T_95b02_row16_col1, #T_95b02_row16_col2, #T_95b02_row16_col3, #T_95b02_row16_col4, #T_95b02_row16_col5, #T_95b02_row16_col6, #T_95b02_row16_col7, #T_95b02_row16_col8, #T_95b02_row16_col9, #T_95b02_row16_col10, #T_95b02_row17_col0, #T_95b02_row17_col1, #T_95b02_row17_col2, #T_95b02_row17_col3, #T_95b02_row17_col4, #T_95b02_row17_col5, #T_95b02_row17_col6, #T_95b02_row17_col7, #T_95b02_row17_col8, #T_95b02_row17_col9, #T_95b02_row17_col10, #T_95b02_row18_col0, #T_95b02_row18_col1, #T_95b02_row18_col2, #T_95b02_row18_col3, #T_95b02_row18_col4, #T_95b02_row18_col5, #T_95b02_row18_col6, #T_95b02_row18_col7, #T_95b02_row18_col8, #T_95b02_row18_col9, #T_95b02_row18_col10, #T_95b02_row19_col0, #T_95b02_row19_col1, #T_95b02_row19_col2, #T_95b02_row19_col3, #T_95b02_row19_col4, #T_95b02_row19_col5, #T_95b02_row19_col6, #T_95b02_row19_col7, #T_95b02_row19_col8, #T_95b02_row19_col9, #T_95b02_row19_col10 {\n","  text-align: left;\n","}\n","</style>\n","<table id=\"T_95b02\" class=\"dataframe\">\n","  <thead>\n","    <tr>\n","      <th id=\"T_95b02_level0_col0\" class=\"col_heading level0 col0\" >token</th>\n","      <th id=\"T_95b02_level0_col1\" class=\"col_heading level0 col1\" >lemma</th>\n","      <th id=\"T_95b02_level0_col2\" class=\"col_heading level0 col2\" >tag</th>\n","      <th id=\"T_95b02_level0_col3\" class=\"col_heading level0 col3\" >case</th>\n","      <th id=\"T_95b02_level0_col4\" class=\"col_heading level0 col4\" >gender</th>\n","      <th id=\"T_95b02_level0_col5\" class=\"col_heading level0 col5\" >mood</th>\n","      <th id=\"T_95b02_level0_col6\" class=\"col_heading level0 col6\" >number</th>\n","      <th id=\"T_95b02_level0_col7\" class=\"col_heading level0 col7\" >person</th>\n","      <th id=\"T_95b02_level0_col8\" class=\"col_heading level0 col8\" >proper</th>\n","      <th id=\"T_95b02_level0_col9\" class=\"col_heading level0 col9\" >tense</th>\n","      <th id=\"T_95b02_level0_col10\" class=\"col_heading level0 col10\" >voice</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td id=\"T_95b02_row0_col0\" class=\"data row0 col0\" >Guido</td>\n","      <td id=\"T_95b02_row0_col1\" class=\"data row0 col1\" ></td>\n","      <td id=\"T_95b02_row0_col2\" class=\"data row0 col2\" >NOUN</td>\n","      <td id=\"T_95b02_row0_col3\" class=\"data row0 col3\" ></td>\n","      <td id=\"T_95b02_row0_col4\" class=\"data row0 col4\" ></td>\n","      <td id=\"T_95b02_row0_col5\" class=\"data row0 col5\" ></td>\n","      <td id=\"T_95b02_row0_col6\" class=\"data row0 col6\" >SINGULAR</td>\n","      <td id=\"T_95b02_row0_col7\" class=\"data row0 col7\" ></td>\n","      <td id=\"T_95b02_row0_col8\" class=\"data row0 col8\" >PROPER</td>\n","      <td id=\"T_95b02_row0_col9\" class=\"data row0 col9\" ></td>\n","      <td id=\"T_95b02_row0_col10\" class=\"data row0 col10\" ></td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_95b02_row1_col0\" class=\"data row1 col0\" >van</td>\n","      <td id=\"T_95b02_row1_col1\" class=\"data row1 col1\" ></td>\n","      <td id=\"T_95b02_row1_col2\" class=\"data row1 col2\" >NOUN</td>\n","      <td id=\"T_95b02_row1_col3\" class=\"data row1 col3\" ></td>\n","      <td id=\"T_95b02_row1_col4\" class=\"data row1 col4\" ></td>\n","      <td id=\"T_95b02_row1_col5\" class=\"data row1 col5\" ></td>\n","      <td id=\"T_95b02_row1_col6\" class=\"data row1 col6\" >SINGULAR</td>\n","      <td id=\"T_95b02_row1_col7\" class=\"data row1 col7\" ></td>\n","      <td id=\"T_95b02_row1_col8\" class=\"data row1 col8\" >PROPER</td>\n","      <td id=\"T_95b02_row1_col9\" class=\"data row1 col9\" ></td>\n","      <td id=\"T_95b02_row1_col10\" class=\"data row1 col10\" ></td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_95b02_row2_col0\" class=\"data row2 col0\" >Rossum</td>\n","      <td id=\"T_95b02_row2_col1\" class=\"data row2 col1\" ></td>\n","      <td id=\"T_95b02_row2_col2\" class=\"data row2 col2\" >NOUN</td>\n","      <td id=\"T_95b02_row2_col3\" class=\"data row2 col3\" ></td>\n","      <td id=\"T_95b02_row2_col4\" class=\"data row2 col4\" ></td>\n","      <td id=\"T_95b02_row2_col5\" class=\"data row2 col5\" ></td>\n","      <td id=\"T_95b02_row2_col6\" class=\"data row2 col6\" >SINGULAR</td>\n","      <td id=\"T_95b02_row2_col7\" class=\"data row2 col7\" ></td>\n","      <td id=\"T_95b02_row2_col8\" class=\"data row2 col8\" >PROPER</td>\n","      <td id=\"T_95b02_row2_col9\" class=\"data row2 col9\" ></td>\n","      <td id=\"T_95b02_row2_col10\" class=\"data row2 col10\" ></td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_95b02_row3_col0\" class=\"data row3 col0\" >is</td>\n","      <td id=\"T_95b02_row3_col1\" class=\"data row3 col1\" >be</td>\n","      <td id=\"T_95b02_row3_col2\" class=\"data row3 col2\" >VERB</td>\n","      <td id=\"T_95b02_row3_col3\" class=\"data row3 col3\" ></td>\n","      <td id=\"T_95b02_row3_col4\" class=\"data row3 col4\" ></td>\n","      <td id=\"T_95b02_row3_col5\" class=\"data row3 col5\" >INDICATIVE</td>\n","      <td id=\"T_95b02_row3_col6\" class=\"data row3 col6\" >SINGULAR</td>\n","      <td id=\"T_95b02_row3_col7\" class=\"data row3 col7\" >THIRD</td>\n","      <td id=\"T_95b02_row3_col8\" class=\"data row3 col8\" ></td>\n","      <td id=\"T_95b02_row3_col9\" class=\"data row3 col9\" >PRESENT</td>\n","      <td id=\"T_95b02_row3_col10\" class=\"data row3 col10\" ></td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_95b02_row4_col0\" class=\"data row4 col0\" >best</td>\n","      <td id=\"T_95b02_row4_col1\" class=\"data row4 col1\" >well</td>\n","      <td id=\"T_95b02_row4_col2\" class=\"data row4 col2\" >ADV</td>\n","      <td id=\"T_95b02_row4_col3\" class=\"data row4 col3\" ></td>\n","      <td id=\"T_95b02_row4_col4\" class=\"data row4 col4\" ></td>\n","      <td id=\"T_95b02_row4_col5\" class=\"data row4 col5\" ></td>\n","      <td id=\"T_95b02_row4_col6\" class=\"data row4 col6\" ></td>\n","      <td id=\"T_95b02_row4_col7\" class=\"data row4 col7\" ></td>\n","      <td id=\"T_95b02_row4_col8\" class=\"data row4 col8\" ></td>\n","      <td id=\"T_95b02_row4_col9\" class=\"data row4 col9\" ></td>\n","      <td id=\"T_95b02_row4_col10\" class=\"data row4 col10\" ></td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_95b02_row5_col0\" class=\"data row5 col0\" >known</td>\n","      <td id=\"T_95b02_row5_col1\" class=\"data row5 col1\" >know</td>\n","      <td id=\"T_95b02_row5_col2\" class=\"data row5 col2\" >VERB</td>\n","      <td id=\"T_95b02_row5_col3\" class=\"data row5 col3\" ></td>\n","      <td id=\"T_95b02_row5_col4\" class=\"data row5 col4\" ></td>\n","      <td id=\"T_95b02_row5_col5\" class=\"data row5 col5\" ></td>\n","      <td id=\"T_95b02_row5_col6\" class=\"data row5 col6\" ></td>\n","      <td id=\"T_95b02_row5_col7\" class=\"data row5 col7\" ></td>\n","      <td id=\"T_95b02_row5_col8\" class=\"data row5 col8\" ></td>\n","      <td id=\"T_95b02_row5_col9\" class=\"data row5 col9\" >PAST</td>\n","      <td id=\"T_95b02_row5_col10\" class=\"data row5 col10\" ></td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_95b02_row6_col0\" class=\"data row6 col0\" >as</td>\n","      <td id=\"T_95b02_row6_col1\" class=\"data row6 col1\" ></td>\n","      <td id=\"T_95b02_row6_col2\" class=\"data row6 col2\" >ADP</td>\n","      <td id=\"T_95b02_row6_col3\" class=\"data row6 col3\" ></td>\n","      <td id=\"T_95b02_row6_col4\" class=\"data row6 col4\" ></td>\n","      <td id=\"T_95b02_row6_col5\" class=\"data row6 col5\" ></td>\n","      <td id=\"T_95b02_row6_col6\" class=\"data row6 col6\" ></td>\n","      <td id=\"T_95b02_row6_col7\" class=\"data row6 col7\" ></td>\n","      <td id=\"T_95b02_row6_col8\" class=\"data row6 col8\" ></td>\n","      <td id=\"T_95b02_row6_col9\" class=\"data row6 col9\" ></td>\n","      <td id=\"T_95b02_row6_col10\" class=\"data row6 col10\" ></td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_95b02_row7_col0\" class=\"data row7 col0\" >the</td>\n","      <td id=\"T_95b02_row7_col1\" class=\"data row7 col1\" ></td>\n","      <td id=\"T_95b02_row7_col2\" class=\"data row7 col2\" >DET</td>\n","      <td id=\"T_95b02_row7_col3\" class=\"data row7 col3\" ></td>\n","      <td id=\"T_95b02_row7_col4\" class=\"data row7 col4\" ></td>\n","      <td id=\"T_95b02_row7_col5\" class=\"data row7 col5\" ></td>\n","      <td id=\"T_95b02_row7_col6\" class=\"data row7 col6\" ></td>\n","      <td id=\"T_95b02_row7_col7\" class=\"data row7 col7\" ></td>\n","      <td id=\"T_95b02_row7_col8\" class=\"data row7 col8\" ></td>\n","      <td id=\"T_95b02_row7_col9\" class=\"data row7 col9\" ></td>\n","      <td id=\"T_95b02_row7_col10\" class=\"data row7 col10\" ></td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_95b02_row8_col0\" class=\"data row8 col0\" >creator</td>\n","      <td id=\"T_95b02_row8_col1\" class=\"data row8 col1\" ></td>\n","      <td id=\"T_95b02_row8_col2\" class=\"data row8 col2\" >NOUN</td>\n","      <td id=\"T_95b02_row8_col3\" class=\"data row8 col3\" ></td>\n","      <td id=\"T_95b02_row8_col4\" class=\"data row8 col4\" ></td>\n","      <td id=\"T_95b02_row8_col5\" class=\"data row8 col5\" ></td>\n","      <td id=\"T_95b02_row8_col6\" class=\"data row8 col6\" >SINGULAR</td>\n","      <td id=\"T_95b02_row8_col7\" class=\"data row8 col7\" ></td>\n","      <td id=\"T_95b02_row8_col8\" class=\"data row8 col8\" ></td>\n","      <td id=\"T_95b02_row8_col9\" class=\"data row8 col9\" ></td>\n","      <td id=\"T_95b02_row8_col10\" class=\"data row8 col10\" ></td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_95b02_row9_col0\" class=\"data row9 col0\" >of</td>\n","      <td id=\"T_95b02_row9_col1\" class=\"data row9 col1\" ></td>\n","      <td id=\"T_95b02_row9_col2\" class=\"data row9 col2\" >ADP</td>\n","      <td id=\"T_95b02_row9_col3\" class=\"data row9 col3\" ></td>\n","      <td id=\"T_95b02_row9_col4\" class=\"data row9 col4\" ></td>\n","      <td id=\"T_95b02_row9_col5\" class=\"data row9 col5\" ></td>\n","      <td id=\"T_95b02_row9_col6\" class=\"data row9 col6\" ></td>\n","      <td id=\"T_95b02_row9_col7\" class=\"data row9 col7\" ></td>\n","      <td id=\"T_95b02_row9_col8\" class=\"data row9 col8\" ></td>\n","      <td id=\"T_95b02_row9_col9\" class=\"data row9 col9\" ></td>\n","      <td id=\"T_95b02_row9_col10\" class=\"data row9 col10\" ></td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_95b02_row10_col0\" class=\"data row10 col0\" >Python</td>\n","      <td id=\"T_95b02_row10_col1\" class=\"data row10 col1\" ></td>\n","      <td id=\"T_95b02_row10_col2\" class=\"data row10 col2\" >NOUN</td>\n","      <td id=\"T_95b02_row10_col3\" class=\"data row10 col3\" ></td>\n","      <td id=\"T_95b02_row10_col4\" class=\"data row10 col4\" ></td>\n","      <td id=\"T_95b02_row10_col5\" class=\"data row10 col5\" ></td>\n","      <td id=\"T_95b02_row10_col6\" class=\"data row10 col6\" >SINGULAR</td>\n","      <td id=\"T_95b02_row10_col7\" class=\"data row10 col7\" ></td>\n","      <td id=\"T_95b02_row10_col8\" class=\"data row10 col8\" >PROPER</td>\n","      <td id=\"T_95b02_row10_col9\" class=\"data row10 col9\" ></td>\n","      <td id=\"T_95b02_row10_col10\" class=\"data row10 col10\" ></td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_95b02_row11_col0\" class=\"data row11 col0\" >.</td>\n","      <td id=\"T_95b02_row11_col1\" class=\"data row11 col1\" ></td>\n","      <td id=\"T_95b02_row11_col2\" class=\"data row11 col2\" >PUNCT</td>\n","      <td id=\"T_95b02_row11_col3\" class=\"data row11 col3\" ></td>\n","      <td id=\"T_95b02_row11_col4\" class=\"data row11 col4\" ></td>\n","      <td id=\"T_95b02_row11_col5\" class=\"data row11 col5\" ></td>\n","      <td id=\"T_95b02_row11_col6\" class=\"data row11 col6\" ></td>\n","      <td id=\"T_95b02_row11_col7\" class=\"data row11 col7\" ></td>\n","      <td id=\"T_95b02_row11_col8\" class=\"data row11 col8\" ></td>\n","      <td id=\"T_95b02_row11_col9\" class=\"data row11 col9\" ></td>\n","      <td id=\"T_95b02_row11_col10\" class=\"data row11 col10\" ></td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_95b02_row12_col0\" class=\"data row12 col0\" >He</td>\n","      <td id=\"T_95b02_row12_col1\" class=\"data row12 col1\" ></td>\n","      <td id=\"T_95b02_row12_col2\" class=\"data row12 col2\" >PRON</td>\n","      <td id=\"T_95b02_row12_col3\" class=\"data row12 col3\" >NOMINATIVE</td>\n","      <td id=\"T_95b02_row12_col4\" class=\"data row12 col4\" >MASCULINE</td>\n","      <td id=\"T_95b02_row12_col5\" class=\"data row12 col5\" ></td>\n","      <td id=\"T_95b02_row12_col6\" class=\"data row12 col6\" >SINGULAR</td>\n","      <td id=\"T_95b02_row12_col7\" class=\"data row12 col7\" >THIRD</td>\n","      <td id=\"T_95b02_row12_col8\" class=\"data row12 col8\" ></td>\n","      <td id=\"T_95b02_row12_col9\" class=\"data row12 col9\" ></td>\n","      <td id=\"T_95b02_row12_col10\" class=\"data row12 col10\" ></td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_95b02_row13_col0\" class=\"data row13 col0\" >was</td>\n","      <td id=\"T_95b02_row13_col1\" class=\"data row13 col1\" >be</td>\n","      <td id=\"T_95b02_row13_col2\" class=\"data row13 col2\" >VERB</td>\n","      <td id=\"T_95b02_row13_col3\" class=\"data row13 col3\" ></td>\n","      <td id=\"T_95b02_row13_col4\" class=\"data row13 col4\" ></td>\n","      <td id=\"T_95b02_row13_col5\" class=\"data row13 col5\" >INDICATIVE</td>\n","      <td id=\"T_95b02_row13_col6\" class=\"data row13 col6\" >SINGULAR</td>\n","      <td id=\"T_95b02_row13_col7\" class=\"data row13 col7\" >THIRD</td>\n","      <td id=\"T_95b02_row13_col8\" class=\"data row13 col8\" ></td>\n","      <td id=\"T_95b02_row13_col9\" class=\"data row13 col9\" >PAST</td>\n","      <td id=\"T_95b02_row13_col10\" class=\"data row13 col10\" ></td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_95b02_row14_col0\" class=\"data row14 col0\" >born</td>\n","      <td id=\"T_95b02_row14_col1\" class=\"data row14 col1\" >bear</td>\n","      <td id=\"T_95b02_row14_col2\" class=\"data row14 col2\" >VERB</td>\n","      <td id=\"T_95b02_row14_col3\" class=\"data row14 col3\" ></td>\n","      <td id=\"T_95b02_row14_col4\" class=\"data row14 col4\" ></td>\n","      <td id=\"T_95b02_row14_col5\" class=\"data row14 col5\" ></td>\n","      <td id=\"T_95b02_row14_col6\" class=\"data row14 col6\" ></td>\n","      <td id=\"T_95b02_row14_col7\" class=\"data row14 col7\" ></td>\n","      <td id=\"T_95b02_row14_col8\" class=\"data row14 col8\" ></td>\n","      <td id=\"T_95b02_row14_col9\" class=\"data row14 col9\" >PAST</td>\n","      <td id=\"T_95b02_row14_col10\" class=\"data row14 col10\" >PASSIVE</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_95b02_row15_col0\" class=\"data row15 col0\" >in</td>\n","      <td id=\"T_95b02_row15_col1\" class=\"data row15 col1\" ></td>\n","      <td id=\"T_95b02_row15_col2\" class=\"data row15 col2\" >ADP</td>\n","      <td id=\"T_95b02_row15_col3\" class=\"data row15 col3\" ></td>\n","      <td id=\"T_95b02_row15_col4\" class=\"data row15 col4\" ></td>\n","      <td id=\"T_95b02_row15_col5\" class=\"data row15 col5\" ></td>\n","      <td id=\"T_95b02_row15_col6\" class=\"data row15 col6\" ></td>\n","      <td id=\"T_95b02_row15_col7\" class=\"data row15 col7\" ></td>\n","      <td id=\"T_95b02_row15_col8\" class=\"data row15 col8\" ></td>\n","      <td id=\"T_95b02_row15_col9\" class=\"data row15 col9\" ></td>\n","      <td id=\"T_95b02_row15_col10\" class=\"data row15 col10\" ></td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_95b02_row16_col0\" class=\"data row16 col0\" >Haarlem</td>\n","      <td id=\"T_95b02_row16_col1\" class=\"data row16 col1\" ></td>\n","      <td id=\"T_95b02_row16_col2\" class=\"data row16 col2\" >NOUN</td>\n","      <td id=\"T_95b02_row16_col3\" class=\"data row16 col3\" ></td>\n","      <td id=\"T_95b02_row16_col4\" class=\"data row16 col4\" ></td>\n","      <td id=\"T_95b02_row16_col5\" class=\"data row16 col5\" ></td>\n","      <td id=\"T_95b02_row16_col6\" class=\"data row16 col6\" >SINGULAR</td>\n","      <td id=\"T_95b02_row16_col7\" class=\"data row16 col7\" ></td>\n","      <td id=\"T_95b02_row16_col8\" class=\"data row16 col8\" >PROPER</td>\n","      <td id=\"T_95b02_row16_col9\" class=\"data row16 col9\" ></td>\n","      <td id=\"T_95b02_row16_col10\" class=\"data row16 col10\" ></td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_95b02_row17_col0\" class=\"data row17 col0\" >,</td>\n","      <td id=\"T_95b02_row17_col1\" class=\"data row17 col1\" ></td>\n","      <td id=\"T_95b02_row17_col2\" class=\"data row17 col2\" >PUNCT</td>\n","      <td id=\"T_95b02_row17_col3\" class=\"data row17 col3\" ></td>\n","      <td id=\"T_95b02_row17_col4\" class=\"data row17 col4\" ></td>\n","      <td id=\"T_95b02_row17_col5\" class=\"data row17 col5\" ></td>\n","      <td id=\"T_95b02_row17_col6\" class=\"data row17 col6\" ></td>\n","      <td id=\"T_95b02_row17_col7\" class=\"data row17 col7\" ></td>\n","      <td id=\"T_95b02_row17_col8\" class=\"data row17 col8\" ></td>\n","      <td id=\"T_95b02_row17_col9\" class=\"data row17 col9\" ></td>\n","      <td id=\"T_95b02_row17_col10\" class=\"data row17 col10\" ></td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_95b02_row18_col0\" class=\"data row18 col0\" >Netherlands</td>\n","      <td id=\"T_95b02_row18_col1\" class=\"data row18 col1\" ></td>\n","      <td id=\"T_95b02_row18_col2\" class=\"data row18 col2\" >NOUN</td>\n","      <td id=\"T_95b02_row18_col3\" class=\"data row18 col3\" ></td>\n","      <td id=\"T_95b02_row18_col4\" class=\"data row18 col4\" ></td>\n","      <td id=\"T_95b02_row18_col5\" class=\"data row18 col5\" ></td>\n","      <td id=\"T_95b02_row18_col6\" class=\"data row18 col6\" >SINGULAR</td>\n","      <td id=\"T_95b02_row18_col7\" class=\"data row18 col7\" ></td>\n","      <td id=\"T_95b02_row18_col8\" class=\"data row18 col8\" >PROPER</td>\n","      <td id=\"T_95b02_row18_col9\" class=\"data row18 col9\" ></td>\n","      <td id=\"T_95b02_row18_col10\" class=\"data row18 col10\" ></td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_95b02_row19_col0\" class=\"data row19 col0\" >.</td>\n","      <td id=\"T_95b02_row19_col1\" class=\"data row19 col1\" ></td>\n","      <td id=\"T_95b02_row19_col2\" class=\"data row19 col2\" >PUNCT</td>\n","      <td id=\"T_95b02_row19_col3\" class=\"data row19 col3\" ></td>\n","      <td id=\"T_95b02_row19_col4\" class=\"data row19 col4\" ></td>\n","      <td id=\"T_95b02_row19_col5\" class=\"data row19 col5\" ></td>\n","      <td id=\"T_95b02_row19_col6\" class=\"data row19 col6\" ></td>\n","      <td id=\"T_95b02_row19_col7\" class=\"data row19 col7\" ></td>\n","      <td id=\"T_95b02_row19_col8\" class=\"data row19 col8\" ></td>\n","      <td id=\"T_95b02_row19_col9\" class=\"data row19 col9\" ></td>\n","      <td id=\"T_95b02_row19_col10\" class=\"data row19 col10\" ></td>\n","    </tr>\n","  </tbody>\n","</table>\n"],"text/plain":["<pandas.io.formats.style.Styler at 0x7f3064b3e500>"]},"metadata":{},"output_type":"display_data"}],"source":["# Input\n","text = \"Guido van Rossum is best known as the creator of Python. He was born in Haarlem, Netherlands.\"  # @param {type:\"string\"}\n","\n","# Send a request to the API\n","analyze_syntax_response = analyze_text_syntax(text)\n","\n","# Show the results\n","show_text_syntax(analyze_syntax_response)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"VmRISErnz7iq"},"source":["There are multiple benefits to extracting the syntax information. One of them is to extract the lemmas. A `lemma` contains the \"root\" word upon which this token is based, which allows you to manage words with their canonical forms.\n","\n","If you dive deeper into the response insights, you'll also find the relationships between the tokens. Here is a visual interpretation showing the complete syntax analysis for this example:\n","\n","![Syntax Analysis](./pics/natural_language_syntax.png)\n","\n","> This is a screenshot from the online [Natural Language demo](https://cloud.google.com/natural-language/#natural-language-api-demo) with which you can create your own parse trees.\n","\n","For more information, see the following:\n","\n","- [`language.AnalyzeSyntaxResponse`](https://cloud.google.com/python/docs/reference/language/latest/google.cloud.language_v1.types.AnalyzeSyntaxResponse)\n","- [Language Support](https://cloud.google.com/natural-language/docs/languages#syntactic_analysis)\n","- [Syntactic analysis](https://cloud.google.com/natural-language/docs/basics#syntactic_analysis)\n","- [Morphology & Dependency Trees](https://cloud.google.com/natural-language/docs/morphology)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"KlDeFxQXdxHw"},"source":["---\n","\n","## 4Ô∏è‚É£ Content classification\n","\n","Content classification analyzes a document and returns a list of content categories that apply to the text found in the document. It is performed with the `classify_text` method which returns a `ClassifyTextResponse`.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q0dk8qaldxHw"},"outputs":[],"source":["def classify_text(text: str) -> language.ClassifyTextResponse:\n","    client = language.LanguageServiceClient()\n","    document = language.Document(\n","        content=text,\n","        type_=language.Document.Type.PLAIN_TEXT,\n","    )\n","    return client.classify_text(document=document)\n","\n","\n","def show_text_classification(response: language.ClassifyTextResponse):\n","    columns = [\"category\", \"confidence\"]\n","    data = ((category.name, category.confidence) for category in response.categories)\n","    formats = {\"confidence\": \"{:.0%}\"}\n","    show_table(columns, data, formats)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"1e6v4_D_xOpC"},"source":["Now, let's perform an analysis:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":112},"executionInfo":{"elapsed":589,"status":"ok","timestamp":1686649995464,"user":{"displayName":"Laurent Picard (GCP Demos)","userId":"15587849602996756367"},"user_tz":-120},"id":"rX5uDw7gbqeW","outputId":"2ca15385-b14b-41a5-d694-b3b07cd799a0"},"outputs":[{"data":{"text/html":["<style type=\"text/css\">\n","#T_48ae4 th {\n","  text-align: center;\n","}\n","#T_48ae4_row0_col0, #T_48ae4_row1_col0 {\n","  text-align: left;\n","}\n","</style>\n","<table id=\"T_48ae4\" class=\"dataframe\">\n","  <thead>\n","    <tr>\n","      <th id=\"T_48ae4_level0_col0\" class=\"col_heading level0 col0\" >category</th>\n","      <th id=\"T_48ae4_level0_col1\" class=\"col_heading level0 col1\" >confidence</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td id=\"T_48ae4_row0_col0\" class=\"data row0 col0\" >/Computers & Electronics/Programming</td>\n","      <td id=\"T_48ae4_row0_col1\" class=\"data row0 col1\" >99%</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_48ae4_row1_col0\" class=\"data row1 col0\" >/Science/Computer Science</td>\n","      <td id=\"T_48ae4_row1_col1\" class=\"data row1 col1\" >99%</td>\n","    </tr>\n","  </tbody>\n","</table>\n"],"text/plain":["<pandas.io.formats.style.Styler at 0x7f3064b3d3f0>"]},"metadata":{},"output_type":"display_data"}],"source":["# Input\n","text = \"Python is an interpreted, high-level, general-purpose programming language. Created by Guido van Rossum and first released in 1991, Python's design philosophy emphasizes code readability with its notable use of significant whitespace.\"  # @param {type:\"string\"}\n","\n","# Send a request to the API\n","classify_text_response = classify_text(text)\n","\n","# Show the results\n","show_text_classification(classify_text_response)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"lmTbxLTcU0DD"},"source":["> Important: You must supply a text block (document) with at least twenty tokens.\n","\n","For more information, see the following docs:\n","\n","- [`ClassifyTextResponse`](https://cloud.google.com/python/docs/reference/language/latest/google.cloud.language_v1.types.ClassifyTextResponse)\n","- [Language Support](https://cloud.google.com/natural-language/docs/languages#content_classification)\n","- [Content Classification](https://cloud.google.com/natural-language/docs/basics#content-classification)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"6kwyisBaPjdy"},"source":["---\n","\n","## 5Ô∏è‚É£ Text moderation (preview)\n","\n","Text moderation analyzes a document against a list of safety attributes, which include \"harmful categories\" and topics that may be considered sensitive. It is performed with the `moderate_text` method which returns a `ModerateTextResponse`.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lCnXARVgQeEG"},"outputs":[],"source":["def moderate_text(text: str) -> language.ModerateTextResponse:\n","    client = language.LanguageServiceClient()\n","    document = language.Document(\n","        content=text,\n","        type_=language.Document.Type.PLAIN_TEXT,\n","    )\n","    return client.moderate_text(document=document)\n","\n","\n","def show_text_moderation(response: language.ModerateTextResponse):\n","    def confidence(category: language.ClassificationCategory) -> float:\n","        return category.confidence\n","\n","    columns = [\"category\", \"confidence\"]\n","    sorted_categories = sorted(\n","        response.moderation_categories, key=confidence, reverse=True\n","    )\n","    data = ((category.name, category.confidence) for category in sorted_categories)\n","    formats = {\"confidence\": \"{:.0%}\"}\n","    show_table(columns, data, formats)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"bYCoQUl2AVyY"},"source":["Now, let's perform an analysis:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":551},"executionInfo":{"elapsed":327,"status":"ok","timestamp":1686649995788,"user":{"displayName":"Laurent Picard (GCP Demos)","userId":"15587849602996756367"},"user_tz":-120},"id":"EAvFP85eAVye","outputId":"b73d714c-88eb-48a5-b889-055803fc91d2"},"outputs":[{"data":{"text/html":["<style type=\"text/css\">\n","#T_f5b18 th {\n","  text-align: center;\n","}\n","#T_f5b18_row0_col0, #T_f5b18_row1_col0, #T_f5b18_row2_col0, #T_f5b18_row3_col0, #T_f5b18_row4_col0, #T_f5b18_row5_col0, #T_f5b18_row6_col0, #T_f5b18_row7_col0, #T_f5b18_row8_col0, #T_f5b18_row9_col0, #T_f5b18_row10_col0, #T_f5b18_row11_col0, #T_f5b18_row12_col0, #T_f5b18_row13_col0, #T_f5b18_row14_col0, #T_f5b18_row15_col0 {\n","  text-align: left;\n","}\n","</style>\n","<table id=\"T_f5b18\" class=\"dataframe\">\n","  <thead>\n","    <tr>\n","      <th id=\"T_f5b18_level0_col0\" class=\"col_heading level0 col0\" >category</th>\n","      <th id=\"T_f5b18_level0_col1\" class=\"col_heading level0 col1\" >confidence</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td id=\"T_f5b18_row0_col0\" class=\"data row0 col0\" >Toxic</td>\n","      <td id=\"T_f5b18_row0_col1\" class=\"data row0 col1\" >68%</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_f5b18_row1_col0\" class=\"data row1 col0\" >Insult</td>\n","      <td id=\"T_f5b18_row1_col1\" class=\"data row1 col1\" >59%</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_f5b18_row2_col0\" class=\"data row2 col0\" >Profanity</td>\n","      <td id=\"T_f5b18_row2_col1\" class=\"data row2 col1\" >52%</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_f5b18_row3_col0\" class=\"data row3 col0\" >Violent</td>\n","      <td id=\"T_f5b18_row3_col1\" class=\"data row3 col1\" >33%</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_f5b18_row4_col0\" class=\"data row4 col0\" >Religion & Belief</td>\n","      <td id=\"T_f5b18_row4_col1\" class=\"data row4 col1\" >27%</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_f5b18_row5_col0\" class=\"data row5 col0\" >Politics</td>\n","      <td id=\"T_f5b18_row5_col1\" class=\"data row5 col1\" >21%</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_f5b18_row6_col0\" class=\"data row6 col0\" >Derogatory</td>\n","      <td id=\"T_f5b18_row6_col1\" class=\"data row6 col1\" >14%</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_f5b18_row7_col0\" class=\"data row7 col0\" >Finance</td>\n","      <td id=\"T_f5b18_row7_col1\" class=\"data row7 col1\" >10%</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_f5b18_row8_col0\" class=\"data row8 col0\" >Legal</td>\n","      <td id=\"T_f5b18_row8_col1\" class=\"data row8 col1\" >10%</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_f5b18_row9_col0\" class=\"data row9 col0\" >Death, Harm & Tragedy</td>\n","      <td id=\"T_f5b18_row9_col1\" class=\"data row9 col1\" >9%</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_f5b18_row10_col0\" class=\"data row10 col0\" >Firearms & Weapons</td>\n","      <td id=\"T_f5b18_row10_col1\" class=\"data row10 col1\" >8%</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_f5b18_row11_col0\" class=\"data row11 col0\" >Illicit Drugs</td>\n","      <td id=\"T_f5b18_row11_col1\" class=\"data row11 col1\" >6%</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_f5b18_row12_col0\" class=\"data row12 col0\" >War & Conflict</td>\n","      <td id=\"T_f5b18_row12_col1\" class=\"data row12 col1\" >5%</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_f5b18_row13_col0\" class=\"data row13 col0\" >Public Safety</td>\n","      <td id=\"T_f5b18_row13_col1\" class=\"data row13 col1\" >5%</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_f5b18_row14_col0\" class=\"data row14 col0\" >Health</td>\n","      <td id=\"T_f5b18_row14_col1\" class=\"data row14 col1\" >3%</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_f5b18_row15_col0\" class=\"data row15 col0\" >Sexual</td>\n","      <td id=\"T_f5b18_row15_col1\" class=\"data row15 col1\" >3%</td>\n","    </tr>\n","  </tbody>\n","</table>\n"],"text/plain":["<pandas.io.formats.style.Styler at 0x7f3064b03130>"]},"metadata":{},"output_type":"display_data"}],"source":["# Input\n","text = \"I have to read Ulysses by James Joyce and am a little over halfway through. I hate it. What a pile of garbage!\"  # @param {type:\"string\"}\n","\n","# Send a request to the API\n","moderate_text_response = moderate_text(text)\n","\n","# Show the results\n","show_text_moderation(moderate_text_response)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"5P_RN-mKAVyf"},"source":["For more information, see the following docs:\n","\n","- [`ModerateTextResponse`](https://cloud.google.com/python/docs/reference/language/latest/google.cloud.language_v1.types.ModerateTextResponse)\n","- [Language Support](https://cloud.google.com/natural-language/docs/languages#content_classification)\n","- [Moderating Text](https://cloud.google.com/natural-language/docs/moderating-text)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"dNqzg9ylC0G2"},"source":["---\n","\n","## üéâ Congratulations\n","\n","You learned how to use the Natural Language API with Python!\n","\n","<center>\n","<table><tr><td>\n","<img src=\"pics/natural_language_api.png\" style=\"height:200px;\" height=\"200\" />\n","</td></tr></table>\n","<table><tr>\n"]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3.10.4 64-bit","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.4"}},"nbformat":4,"nbformat_minor":0}
